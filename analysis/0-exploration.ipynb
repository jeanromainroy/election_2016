{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "from twython import Twython, TwythonError, TwythonRateLimitError\n",
    "import libs.preprocessor as tweet_preproc\n",
    "\n",
    "# Init Preprocessor\n",
    "twitterPreprocessor = tweet_preproc.TwitterPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows in a file\n",
    "def file_len(fname):\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    \n",
    "    nbrOfLines = i + 1\n",
    "    print(\"Nbr of lines : \" + str(nbrOfLines))\n",
    "    \n",
    "    return nbrOfLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *\n",
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, oauth_token, oauth_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traverse Source Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The election-filter*.txt dataset only contain the tweet id. We have no idea when these tweets were posted. For this reason, we traverse at regular increment all the ids in all the datasets and query their created_at attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_ids(filename,\n",
    "                 database_name,\n",
    "                 nb_requests=500):  # less than 900 because many tweet lookup fail\n",
    "    \n",
    "    print(\" --- \" + filename + \" --- \")\n",
    "    \n",
    "    # Count number of lines in file\n",
    "    linesCount = file_len(filename)\n",
    "    \n",
    "    # Opening the ID File:\n",
    "    file = open(filename, \"r\")\n",
    "    \n",
    "    # compute the lookup increment\n",
    "    lookup_inc = round(linesCount/(1.0*nb_requests))\n",
    "    \n",
    "    # Creation of the file that will contain the hydrated tweets:\n",
    "    with open(database_name, 'w+', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        # First line write the headers\n",
    "        csvfile.write(\"row,created_at\\n\")\n",
    "        \n",
    "        # Go through row\n",
    "        row_counter = 0\n",
    "        retryNext = False\n",
    "        for row in file:\n",
    "\n",
    "            # Do every inc\n",
    "            if((row_counter % lookup_inc) == 0 or retryNext == True):\n",
    "\n",
    "                # Reset\n",
    "                retryNext = False\n",
    "\n",
    "                # Strip\n",
    "                row = str(row).strip()\n",
    "                \n",
    "                try:\n",
    "                    # Get Status\n",
    "                    status = twitter.show_status(id=row)\n",
    "                    \n",
    "                    # Get created_at\n",
    "                    created_at = status['created_at']\n",
    "\n",
    "                    # Parse it\n",
    "                    created_at = twitterPreprocessor.to_datetime(created_at)\n",
    "\n",
    "                    # Append to csv\n",
    "                    csvfile.write(str(row_counter) + \",\" + str(created_at) + \"\\n\")\n",
    "                    \n",
    "\n",
    "                except TwythonError as e:\n",
    "                    \n",
    "                    retryNext = True\n",
    "                    \n",
    "                    if isinstance(e, TwythonRateLimitError):\n",
    "                        \n",
    "                        print(\"sleeping for 15 min and 30 seconds\")\n",
    "                        time.sleep(930)\n",
    "                \n",
    "                \n",
    "            # increment row counter\n",
    "            row_counter = row_counter + 1\n",
    "            \n",
    "                     \n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = [\"sources/general/election-filter1.txt\",\"sources/general/election-filter2.txt\",\"sources/general/election-filter3.txt\",\"sources/general/election-filter4.txt\",\"sources/general/election-filter5.txt\",\"sources/general/election-filter6.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = 0\n",
    "\n",
    "database_name = \"sources/general/traversed/election-filter\" + str(collection_id+1) + \".txt\";\n",
    "\n",
    "traverse_ids(collections[collection_id],database_name,nb_requests=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Traverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Grab all the traversed files\n",
    "filenames = glob('sources/general/traversed/traverse/*')\n",
    "\n",
    "# Merged output file\n",
    "output_name = \"databases/all_traverse.csv\"\n",
    "\n",
    "with open(output_name, 'w+') as outfile:\n",
    "    \n",
    "    # Write header\n",
    "    outfile.write(\"file,row,created_at\\n\")\n",
    "    \n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            \n",
    "            name = fname.split(\"/\")[-1]\n",
    "            skipFirst = True\n",
    "            \n",
    "            for line in infile:\n",
    "                \n",
    "                if(skipFirst):\n",
    "                    skipFirst = False\n",
    "                    continue\n",
    "                \n",
    "                line = line.strip()\n",
    "                line = name + \",\" + line + \"\\n\"\n",
    "                outfile.write(line)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered\n",
    "ordered_output_name = \"databases/all_traverse_ordered.csv\"\n",
    "\n",
    "def x_order(x):\n",
    "    \n",
    "    x_createdAt = x['created_at']\n",
    "    x_row = int(x['row'])\n",
    "    \n",
    "    toDigit = x_row/(10**12)\n",
    "    \n",
    "    x_createdAt = x_createdAt.split(\"-\")\n",
    "    x_createdAt = int(x_createdAt[2]) + int(x_createdAt[1])*100 + int(x_createdAt[0])*10000\n",
    "    \n",
    "    orderX = x_createdAt + toDigit\n",
    "    \n",
    "    return orderX\n",
    "    \n",
    "\n",
    "with open(ordered_output_name, 'w+') as outfile:\n",
    "    with open(output_name, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        # Write header\n",
    "        outfile.write(\"file,row,created_at\\n\")\n",
    "        \n",
    "        # init reader\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_createdAt = header.index('created_at')\n",
    "        ind_row = header.index('row')\n",
    "        ind_file = header.index('file')\n",
    "        \n",
    "        # go through rows\n",
    "        allData = []\n",
    "        for row in reader:\n",
    "            \n",
    "            # apppend\n",
    "            allData.append({\n",
    "                \"created_at\":row[ind_createdAt],\n",
    "                \"row\": row[ind_row],\n",
    "                \"file\": row[ind_file]\n",
    "            })\n",
    "            \n",
    "        \n",
    "        # Sort\n",
    "        sortedData = sorted(allData, key=lambda x: x_order(x))\n",
    "        \n",
    "        # Write to output file\n",
    "        for datum in sortedData:\n",
    "            \n",
    "            line = datum['file'] + \",\" + datum['row'] + \",\" + datum['created_at'] + \"\\n\"\n",
    "            outfile.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
