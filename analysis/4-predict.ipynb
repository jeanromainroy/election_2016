{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import libs.bag_of_worder as bag_of_worder\n",
    "import libs.preprocessor as tweet_preproc\n",
    "\n",
    "# Keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# --- CNN ---\n",
    "try:\n",
    "    cnn_clf = load_model('model/dl_cnn.h5')\n",
    "    print(\"CNN classifier loaded!\")\n",
    "except:\n",
    "    print(\"ERROR: CNN not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Preprocessor\n",
    "twitterPreprocessor = tweet_preproc.TwitterPreprocessor()\n",
    "\n",
    "# Keras Tokenizer\n",
    "with open('model/tokenizer.joblib', 'rb') as handle:\n",
    "    tokenizer = load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformTweets(tweets):\n",
    "    \n",
    "    # Convert to sequence\n",
    "    tweets_t = tokenizer.texts_to_sequences(tweets)\n",
    "    \n",
    "    # Adding 1 because of reserved 0 index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Max sequence length\n",
    "    maxlen = 100\n",
    "\n",
    "    # Pad\n",
    "    tweets_t = pad_sequences(tweets_t, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return tweets_t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(tweets, max_thresh=0.5, min_thresh=0.5):\n",
    "    \n",
    "    # Transform tweets \n",
    "    tweets = transformTweets(tweets)\n",
    "\n",
    "    # Predict\n",
    "    preds = cnn_clf.predict(tweets)\n",
    "    \n",
    "    # Make sure they are the same size\n",
    "    assert len(preds) == len(tweets)\n",
    "\n",
    "    # Convert to binary\n",
    "    binary_preds = []\n",
    "    for pred in preds:\n",
    "        if(pred > max_thresh):\n",
    "            binary_preds.append(1)\n",
    "            \n",
    "        elif(pred < min_thresh):\n",
    "            binary_preds.append(0)\n",
    "            \n",
    "    return binary_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    \n",
    "    nbrOfLines = i + 1\n",
    "    print(\"Nbr of lines : \" + str(nbrOfLines))\n",
    "    \n",
    "    return nbrOfLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictFile(src_path,out_path):\n",
    "    \n",
    "    # Count number of tweets\n",
    "    nbr_tweets = file_len(src_path)\n",
    "\n",
    "    with open(out_path, 'w+', newline='', encoding=\"utf-8\") as outfile:\n",
    "        with open(src_path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "            # init reader\n",
    "            reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "\n",
    "            # Taking the header of the file + the index of useful columns:\n",
    "            header = next(reader)\n",
    "            ind_createdAt = header.index('created_at')\n",
    "            ind_text = header.index('text')\n",
    "            ind_description = header.index('description')\n",
    "            ind_location = header.index('location')\n",
    "\n",
    "            # convert tweets file to list                \n",
    "            tweets = []\n",
    "            all_tweets = []\n",
    "            for row in reader:\n",
    "                    \n",
    "                # get data\n",
    "                created_at = row[ind_createdAt]\n",
    "                text = row[ind_text]\n",
    "                #description = row[ind_description]\n",
    "                location = row[ind_location]\n",
    "                \n",
    "                # append to lists\n",
    "                all_tweets.append([created_at,text,location])\n",
    "                tweets.append(text)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Predict all tweets\n",
    "            preds = predict_cnn(tweets, max_thresh=0.95, min_thresh=0.05)\n",
    "            \n",
    "            # Write headers for first row\n",
    "            outfile.write('\"label\",\"created_at\",\"text\",\"location\"\\n')\n",
    "\n",
    "            # Init counter\n",
    "            tweet_counter = 0\n",
    "            \n",
    "            # Write to file\n",
    "            for i in tqdm(range(0,len(preds))):\n",
    "                \n",
    "                # Get pred\n",
    "                pred = preds[i]\n",
    "\n",
    "                # If failed skip\n",
    "                if(pred < 0):\n",
    "                    continue\n",
    "                    \n",
    "                # Get data                \n",
    "                created_at = all_tweets[i][0]\n",
    "                text = all_tweets[i][1]\n",
    "                #description = row[ind_description]\n",
    "                location = all_tweets[i][2]\n",
    "\n",
    "                # Write to file\n",
    "                rowData = [str(pred),created_at,text,location]\n",
    "                rowData = '\"' + '\",\"'.join(rowData) + '\"\\n'\n",
    "                outfile.write(rowData)\n",
    "\n",
    "                # increment counter\n",
    "                tweet_counter = tweet_counter + 1\n",
    "                    \n",
    "                \n",
    "            print(\"Nbr of tweets labeled: \" + str(tweet_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glob all the tweets csv\n",
    "filenames = glob(\"data/general/*/tweets.csv\")\n",
    "for fname in filenames:\n",
    "    \n",
    "    print(fname)\n",
    "    outpath = \"/\".join(fname.split(\"/\")[:-1]) + \"/predictions.csv\"\n",
    "    predictFile(fname, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
