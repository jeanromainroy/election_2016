{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we build our dictionary and train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the candidates and parties preprocessed tweets as our labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libs.bag_of_worder as bag_of_worder\n",
    "import libs.preprocessor as tweet_preproc\n",
    "\n",
    "# Init Preprocessor\n",
    "twitterPreprocessor = tweet_preproc.TwitterPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT, the label should have only two states 0: dems, 1: republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('label')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            if label == \"democrat\":\n",
    "                y.append(0)\n",
    "            elif label == \"republican\":\n",
    "                y.append(1)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBalanced(labels):\n",
    "    return np.count_nonzero(labels)/len(labels)\n",
    "\n",
    "\n",
    "def balance_dataset(features, labels):\n",
    "    \n",
    "    # Combine the features with the labels\n",
    "    combined = list(zip(features, labels))\n",
    "    \n",
    "    # Shuffle the list\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    # Split the feature and label\n",
    "    X[:], y[:] = zip(*combined)\n",
    "    \n",
    "    # Count number of '1'\n",
    "    nbrOfOnes = np.count_nonzero(y)\n",
    "    nbrOfZeros = len(y) - nbrOfOnes\n",
    "    excessNbr = abs(nbrOfOnes - nbrOfZeros)\n",
    "    \n",
    "    # Balance dataset\n",
    "    removed_counter = 0\n",
    "    if(nbrOfOnes > nbrOfZeros):  # too much '1'\n",
    "\n",
    "        for ind in range(0,len(y)):\n",
    "            if(y[ind] == 1):\n",
    "                X.pop(ind)\n",
    "                y.pop(ind)\n",
    "                removed_counter = removed_counter + 1\n",
    "                \n",
    "            if(removed_counter >= abs(excessNbr)):\n",
    "                break\n",
    "\n",
    "    else:                       # too much '0'\n",
    "\n",
    "        for ind in range(0,len(y)):\n",
    "            if(y[ind] == 0):\n",
    "                X.pop(ind)\n",
    "                y.pop(ind)\n",
    "                removed_counter = removed_counter + 1\n",
    "                \n",
    "            if(removed_counter >= abs(excessNbr)):\n",
    "                break\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the labeled dataset\n",
    "path = \"data/parties_candidates/sources.csv\"\n",
    "\n",
    "# Load dataset from path\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "# Make sure there is 50/50 of both labels\n",
    "X, y = balance_dataset(X,y)\n",
    "\n",
    "# Split the data\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(X, y, test_size=0.1, random_state=12, shuffle=True, stratify=y)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the training data is balanced\n",
    "print(\"--- Proportion of republican label ---\")\n",
    "print(checkBalanced(train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Length in Words\n",
    "Inspired from: https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(np.unique(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(train_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in train_X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "fig = plt.figure()\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing models\n",
    "Codes copied from: https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "\n",
    "train_X_t = tokenizer.texts_to_sequences(train_X)\n",
    "valid_X_t = tokenizer.texts_to_sequences(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "train_X_t = pad_sequences(train_X_t, padding='post', maxlen=maxlen)\n",
    "valid_X_t = pad_sequences(valid_X_t, padding='post', maxlen=maxlen)\n",
    "\n",
    "print('vocab_size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_t[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('model/tokenizer.joblib', 'wb') as handle:\n",
    "    dump(tokenizer, handle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest algorithm\n",
    "Here, we don't need any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(train_X_t, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(valid_X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(valid_Y, predictions))\n",
    "print(classification_report(valid_Y, predictions))\n",
    "print(accuracy_score(valid_Y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('model/rand_forest.joblib', 'wb') as handle:\n",
    "    dump(text_classifier, handle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_X_t_n = scaler.fit_transform(train_X_t)\n",
    "valid_X_t_n = scaler.transform(valid_X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_classifier = LogisticRegression()\n",
    "text_classifier.fit(train_X_t, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(valid_X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(valid_Y, predictions))\n",
    "print(classification_report(valid_Y, predictions))\n",
    "print(accuracy_score(valid_Y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('model/logistic.joblib', 'wb') as handle:\n",
    "    dump(text_classifier, handle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "text_classifier = svm.SVC(gamma='auto')\n",
    "text_classifier.fit(train_X_t_n, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(valid_X_t_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(valid_Y, predictions))\n",
    "print(classification_report(valid_Y, predictions))\n",
    "print(accuracy_score(valid_Y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(text_classifier, 'model/svm_svc.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network\n",
    "Inspired from: https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X_t, train_Y, batch_size=128, epochs=5, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/dl_snn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_X_t, valid_Y, verbose=0)\n",
    "\n",
    "print('Test Score: {}'.format(score[0]))\n",
    "print('Test Accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(valid_X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(valid_Y, predictions.round()))\n",
    "print(classification_report(valid_Y, predictions.round()))\n",
    "print(accuracy_score(valid_Y, predictions.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Dimensional Convolutional Neural Network Model\n",
    "Inspired from: https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 256, input_length=maxlen))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X_t, train_Y, batch_size=128, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/dl_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_X_t, valid_Y, verbose=0)\n",
    "\n",
    "print('Test Score: {}'.format(score[0]))\n",
    "print('Test Accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('results/dl_cnn_train_curve_acc.pdf', bbox_inches='tight')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('results/dl_cnn_train_curve_loss.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(valid_X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(valid_Y, predictions.round()))\n",
    "print(classification_report(valid_Y, predictions.round()))\n",
    "print(accuracy_score(valid_Y, predictions.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
