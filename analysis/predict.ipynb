{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import libs.bag_of_worder as bag_of_worder\n",
    "import libs.preprocessor as tweet_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Load Model\n",
    "try:\n",
    "    classifier = load('model/logistic.joblib') \n",
    "    print(\"Model loaded!\")\n",
    "\n",
    "except:\n",
    "    print(\"ERROR: Model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dict\n",
    "wordDict = []\n",
    "\n",
    "path = \"model/dictionary.txt\"\n",
    "with open(path, 'r', newline='', encoding=\"utf-8\") as input_file:    \n",
    "    for row in input_file:\n",
    "        wordDict.append(row.strip())\n",
    "\n",
    "# Get the stats\n",
    "print(\"Dict Dimension: \" + str(len(wordDict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Preprocessor\n",
    "twitterPreprocessor = tweet_preproc.TwitterPreprocessor()\n",
    "\n",
    "# Init Bag-of-Worder using the dictionary\n",
    "countBoW = bag_of_worder.BagOfWorder(wordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTweets(tweets):\n",
    "    \n",
    "    preds = []\n",
    "    for tweet in tweets:\n",
    "        preds.append(predictTweet(tweet))\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def predictTweet(tweet,min_confidence=0.5):\n",
    "    \n",
    "    # Preprocess\n",
    "    tweet = twitterPreprocessor.preprocess(tweet)\n",
    "    \n",
    "    # Create a one hot matrix of the words in the tweet\n",
    "    oneHotTweet = countBoW.computeLine(tweet)\n",
    "    \n",
    "    # Check performance\n",
    "    prob_dem, prob_rep = classifier.predict_proba(oneHotTweet)[0]\n",
    "    \n",
    "    # Compare to min confidence level\n",
    "    if(prob_dem > min_confidence):\n",
    "        return 0\n",
    "    elif(prob_rep > min_confidence):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/general/2016-10-21/tweets.csv\"\n",
    "\n",
    "MIN_CONFIDENCE = 0.8\n",
    "\n",
    "with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "    \n",
    "    # init reader\n",
    "    reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "\n",
    "    # Taking the header of the file + the index of useful columns:\n",
    "    header = next(reader)\n",
    "    ind_createdAt = header.index('created_at')\n",
    "    ind_text = header.index('text')\n",
    "    ind_description = header.index('description')\n",
    "    ind_location = header.index('location')\n",
    "    \n",
    "    # Init counter\n",
    "    tweet_counter = 0\n",
    "\n",
    "    # go through rows\n",
    "    for row in reader:\n",
    "        \n",
    "        # get data\n",
    "        created_at = row[ind_createdAt]\n",
    "        text = row[ind_text]\n",
    "        description = row[ind_description]\n",
    "        location = row[ind_location]\n",
    "        \n",
    "        # predict\n",
    "        print(predictTweet(text,min_confidence=MIN_CONFIDENCE))\n",
    "        #print(predictTweet(description,min_confidence=MIN_CONFIDENCE))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(text)\n",
    "        #print(description)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # increment counter\n",
    "        tweet_counter = tweet_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
