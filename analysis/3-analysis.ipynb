{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we build our dictionary and train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the candidates and parties preprocessed tweets as our labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libs.bag_of_worder as bag_of_worder\n",
    "import libs.preprocessor as tweet_preproc\n",
    "\n",
    "# Init Preprocessor\n",
    "twitterPreprocessor = tweet_preproc.TwitterPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT, the label should have only two states 0: dems, 1: republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('label')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            if label == \"democrat\":\n",
    "                y.append(0)\n",
    "            elif label == \"republican\":\n",
    "                y.append(1)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBalanced(labels):\n",
    "    return np.count_nonzero(labels)/len(labels)\n",
    "\n",
    "\n",
    "def balance_dataset(features, labels):\n",
    "    \n",
    "    # Combine the features with the labels\n",
    "    combined = list(zip(features, labels))\n",
    "    \n",
    "    # Shuffle the list\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    # Split the feature and label\n",
    "    X[:], y[:] = zip(*combined)\n",
    "    \n",
    "    # Count number of '1'\n",
    "    nbrOfOnes = np.count_nonzero(y)\n",
    "    nbrOfZeros = len(y) - nbrOfOnes\n",
    "    excessNbr = abs(nbrOfOnes - nbrOfZeros)\n",
    "    \n",
    "    # Balance dataset\n",
    "    removed_counter = 0\n",
    "    if(nbrOfOnes > nbrOfZeros):  # too much '1'\n",
    "\n",
    "        for ind in range(0,len(y)):\n",
    "            if(y[ind] == 1):\n",
    "                X.pop(ind)\n",
    "                y.pop(ind)\n",
    "                removed_counter = removed_counter + 1\n",
    "                \n",
    "            if(removed_counter >= abs(excessNbr)):\n",
    "                break\n",
    "\n",
    "    else:                       # too much '0'\n",
    "\n",
    "        for ind in range(0,len(y)):\n",
    "            if(y[ind] == 0):\n",
    "                X.pop(ind)\n",
    "                y.pop(ind)\n",
    "                removed_counter = removed_counter + 1\n",
    "                \n",
    "            if(removed_counter >= abs(excessNbr)):\n",
    "                break\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the labeled dataset\n",
    "path = \"data/parties_candidates/sources.csv\"\n",
    "\n",
    "# Load dataset from path\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "# Make sure there is 50/50 of both labels\n",
    "X, y = balance_dataset(X,y)\n",
    "\n",
    "# Split the data\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(X, y, test_size=0.1, random_state=12, shuffle=True, stratify=y)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the training data is balanced\n",
    "print(\"--- Proportion of republican label ---\")\n",
    "print(checkBalanced(train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Init array\n",
    "    bigrams = []\n",
    "    \n",
    "    # Go through tokens\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigrams.append(\" \".join([tokens[i],tokens[i+1]]))\n",
    "    \n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "# Returns unique words\n",
    "def buildDict(tweets, addBigram=False):\n",
    "    \n",
    "    # Init empty set\n",
    "    wordDict = set()\n",
    "    \n",
    "    # Go through each tweet of the validation set\n",
    "    for tweet in tweets:\n",
    "\n",
    "        # Tokenize\n",
    "        words = word_tokenize(tweet)\n",
    "        \n",
    "        # Add Bigram\n",
    "        if(addBigram):\n",
    "            words = words + bigram(words)\n",
    "\n",
    "        # Go through each word\n",
    "        for word in words:\n",
    "\n",
    "            # Append to dictionary if not already there\n",
    "            if(word not in wordDict):\n",
    "                wordDict.add(word)\n",
    "                \n",
    "    # Get the stats\n",
    "    print(\"Dict Dimension: \" + str(len(wordDict)))\n",
    "    \n",
    "    return list(wordDict)\n",
    "\n",
    "\n",
    "def loadDict():\n",
    "    \n",
    "    # Init dict\n",
    "    wordDict = []\n",
    "    \n",
    "    path = \"model/dictionary.txt\"\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as input_file:    \n",
    "        for row in input_file:\n",
    "            wordDict.append(row.strip())\n",
    "            \n",
    "    # Get the stats\n",
    "    print(\"Dict Dimension: \" + str(len(wordDict)))\n",
    "            \n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of all the words\n",
    "wordDict = loadDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduceDict(wordDict,countBoW,tweets):\n",
    "\n",
    "    # Init the BoW Matrix\n",
    "    matrixBoW = np.zeros((1, len(wordDict)),dtype=np.int16)\n",
    "\n",
    "    for tweet in tqdm(tweets):\n",
    "\n",
    "        # Compute the BoW\n",
    "        bowObject = countBoW.computeLine(tweet)\n",
    "\n",
    "        # Add to matrixBoW\n",
    "        matrixBoW = np.add(matrixBoW,bowObject)\n",
    "\n",
    "\n",
    "    # Only keep words that occured more than once\n",
    "    newWordDict = []\n",
    "    for ind in np.argwhere(matrixBoW > 1):\n",
    "        newWordDict.append(wordDict[ind[1]])\n",
    "\n",
    "    reduction = len(newWordDict)/(1.0*len(wordDict))\n",
    "    print(reduction)\n",
    "\n",
    "    \n",
    "    # Write to file\n",
    "    path = \"model/dictionary_new.txt\"\n",
    "    with open(path, 'w+', newline='', encoding=\"utf-8\") as output_file:\n",
    "        for word in newWordDict:\n",
    "            output_file.write(str(word) + \"\\n\")\n",
    "            \n",
    "    print(\"New dictionary created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceDict(wordDict,countBoW,train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Load Model\n",
    "def loadModel():\n",
    "    \n",
    "    try:\n",
    "        classifier = load('model/logistic.joblib') \n",
    "        print(\"Model Loaded!\")\n",
    "        return classifier\n",
    "    \n",
    "    except:\n",
    "        print(\"ERROR: Model not saved\")\n",
    "        \n",
    "        \n",
    "def saveModel(clf):\n",
    "\n",
    "    dump(clf, 'model/logistic.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1,solver='lbfgs', multi_class='auto')\n",
    "\n",
    "    training_rep = bowObj.computeTFID(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.computeTFID(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Bag-of-Worder using the dictionary\n",
    "countBoW = bag_of_worder.BagOfWorder(wordDict)\n",
    "\n",
    "# Train\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X,train_Y,valid_X,valid_Y,countBoW)\n",
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" + str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "saveModel(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPerf(classifier, countBoW):\n",
    "    \n",
    "    # Check performance\n",
    "    preds = classifier.predict(countBoW.computeTFID(valid_X))\n",
    "    print(np.count_nonzero(np.equal(preds,valid_Y))/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkPerf(classifier,countBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
