{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Data Mining\n",
    "\n",
    "\n",
    "## TP2 Autumn 2019 - Extraction and analysis of a tweets database\n",
    "\n",
    "##### Team members:\n",
    "\n",
    "    - Jean-Romain Roy (1720495)\n",
    "    - Pankaj Raj Roy (Matricule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "In 2017, Twitter had 313 million active users per month with 500 million tweets sent per day. This information is made available to web research and development through a public API that collects the information you want.\n",
    "\n",
    "Nevertheless, Twitter's development policy limits the sharing of this data. Indeed, sharing the content of tweets in a database is not allowed, only the tweets identifiers are. To publicly share a tweets database that has been created, it is necessary that this database be made up only of tweets identifiers, and this is what is found in most public data sets.\n",
    "\n",
    "It is therefore necessary to \"hydrate\" the tweets in question, i.e. extract all the information from the ID, which requires using the Twitter API.\n",
    "\n",
    "We will use here public databases created by GWU (George Washington University), which have the advantage of being very recent:\n",
    "https://dataverse.harvard.edu/dataverse/gwu-libraries\n",
    "\n",
    "Each GWU database covers a specific topic (2016 U.S. election, Olympic Games, etc.), and the data was collected by applying queries that filtered the results to have only relevant tweets. A README file is provided with each database to give details of the creation of the *dataset*. \n",
    "\n",
    "**Thus, the objectives of this TP are as follows:**\n",
    "\n",
    " 1. Build a crawler that collects tweet information from its ID, with the dataset of its choice and the relevant information for the chosen subject\n",
    " 2. From these collected Twitter data, application of Machine Learning (ML)/Natural Language Processing (NLP) methods to provide relevant analysis. \n",
    "\n",
    "Twitter allowing the **local** sharing of data (for example within a research group), a database will be provided if you are unable to create your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/ Hydration of tweets using the Twitter API (4 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting Twitter authorization to use the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For authentication, Twitter uses OAuth : https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "You will need OAuth2 in particular here, because you will not interact with users on Twitter (simply collected data).\n",
    "\n",
    "##### 1.1. Obtaining a developper account\n",
    "\n",
    " The first step required to register your application and create a Twitter developer account. To do this:\n",
    "\n",
    " - Create a basic Twitter account (if you don't have one)\n",
    " \n",
    " - On the website https://developer.twitter.com, click on *apply* to obtain a developer account. \n",
    " \n",
    " - Fill in all the necessary fields. Twitter asks for a lot of details on how you will use this account, so it is important to explain the approach in detail: it is important to underline the fact that the project is **academic** (no commercial intention, no publication of the data collected, etc.), explain the objectives and learning of this TP (familiarisation with the Twitter API, the concrete application of Data Mining methods, etc.), but also explain in detail what you will do with the data, the methods you will apply, the report provided, etc.  If you are not specific enough, Twitter can send you an email to ask for clarification. \n",
    "\n",
    "\n",
    "##### 1.2. Obtaining access tokens \n",
    "\n",
    " - When Twitter has validated your developer account request, go to https://developer.twitter.com/en/apps to create an app.\n",
    "\n",
    "- Again, information is to be provided here. Some, like the name or the website, are not very important, you can put a fake website if you want.\n",
    "\n",
    "- At the end of this process, you can finally get the keys and tokens to use the API: go to the application page to create the tokens. You must retrieve a key pair and a pair of tokens to move on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. First steps with Twython\n",
    "\n",
    "##### 2.1 Installation and import of the library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several Python libraries exist to manipulate the Twitter API. Also called wrappers, they are a set of python functions that call API functions. Among them, we will use Twython, a widespread and actively maintained library.\n",
    "\n",
    "Twython documentation : https://twython.readthedocs.io/en/latest/api.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from twython import Twython, TwythonError, TwythonRateLimitError\n",
    "except ImportError:\n",
    "    !pip install twython --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Creation of an app and first test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, oauth_token, oauth_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Custom Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.states import states\n",
    "from libs.states import findState as extract_place\n",
    "\n",
    "import libs.preprocessor as tweet_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Preprocessor\n",
    "twitterPreprocessor = tweet_preproc.TwitterPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows in a file\n",
    "def file_len(fname):\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    \n",
    "    nbrOfLines = i + 1\n",
    "    print(\"Nbr of lines : \" + str(nbrOfLines))\n",
    "    \n",
    "    return nbrOfLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multiple text documents\n",
    "def concatDocuments(input_names,output_name):\n",
    "    \n",
    "    with open(output_name, 'w') as outfile:\n",
    "        for fname in input_names:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. Display the date, username and content of the tweet with the ID : 1157345692517634049\n",
    "\n",
    "*Hint : you can use the twython function `show_status`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_id = \"1157345692517634049\"\n",
    "\n",
    "status = twitter.show_status(id=test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald J. Trump - Fri Aug 02 17:41:30 +0000 2019\n",
      "\n",
      "A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week, get home ASAP A$AP!\n"
     ]
    }
   ],
   "source": [
    "print(str(status['user']['name']) + \" - \" + str(status['created_at']) + \"\\n\")\n",
    "print(str(status['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** Twitter has a 15-minute window request limit, which is therefore to be taken into account in the database:  https://developer.twitter.com/en/docs/basics/rate-limiting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hydratation of a tweets database\n",
    "\n",
    "The serious business is starting!\n",
    "\n",
    "We now want to build a `hydrate_database` function that, from a text file containing a list of tweets IDs, creates a csv file containing the information we want to extract.\n",
    "\n",
    "Due to the request limitation, the `show_status` function seen above is not very effective for this task: at 900 requests for 15 minutes, it will take far too long to build an interesting database. The `lookup_status` function (see documentation) will therefore be more suitable. It will allow to hydrate 100 tweets per request, which, with a limit of 900 requests for 15 minutes, makes the construction of the database more realistic. It will still be necessary to manage the error generated by the limitation, if you want to have more than 90000 tweets or if you call the function several times in less than 15 minutes.\n",
    "\n",
    "#### Question 2. Implement the method `hydrate_database`\n",
    "\n",
    "*Warning : It is also necessary to manage the case where the requested feature is not a dictionary key but a subkey, as is the case for the user name for example.*\n",
    "\n",
    "*Hint : The `sleep` function of time library allows to wait the desire time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hydrate_database(filename, database_name, features, \n",
    "                     checkLocation=True,\n",
    "                     nb_requests=899, \n",
    "                     tweet_hydratation_limit=100,\n",
    "                     start_index=0):\n",
    "    \"\"\"\n",
    "    Create a csv file that contains features of tweets from an file that contains ID of tweets.\n",
    "    \n",
    "    filename: Name of the file that contains ids\n",
    "    database_name: name of the file that will be created\n",
    "    features: List of features\n",
    "    nb_requests: number of time the function lookup_status will be called\n",
    "    tweet_hydratation_limit: Ids bundle length for each request\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Count number of lines in file\n",
    "    linesCount = file_len(filename)\n",
    "    fracOfLines = round(10000.0*(nb_requests*tweet_hydratation_limit)/(1.0*linesCount))/100.0\n",
    "    print(\"Fraction about to be Hydrated : \" + str(fracOfLines))\n",
    "\n",
    "    # Opening the ID File:\n",
    "    file = open(filename, \"r\")\n",
    "    \n",
    "    # Creation of the file that will contain the hydrated tweets:\n",
    "    with open(database_name, 'w+', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        # First line write the headers\n",
    "        headers = []\n",
    "        for feature in features:\n",
    "            if(len(feature) == 1):\n",
    "                headers.append(feature[0])\n",
    "            elif(len(feature) == 2):\n",
    "                headers.append(feature[1])\n",
    "        \n",
    "        headers = ','.join(headers) + '\\n'\n",
    "        csvfile.write(headers)\n",
    "        \n",
    "        ids = []\n",
    "        row_counter = 0\n",
    "        req_counter = 0\n",
    "        for row in file:\n",
    "            \n",
    "            # increment row counter\n",
    "            row_counter = row_counter + 1\n",
    "\n",
    "            # Pagination skip\n",
    "            if(row_counter < start_index):\n",
    "                continue\n",
    "            \n",
    "            # Strip\n",
    "            row = row.strip()\n",
    "            \n",
    "            # Append the id on the row\n",
    "            ids.append(row)\n",
    "            \n",
    "\n",
    "            # If we have our bundle\n",
    "            if((len(ids) % tweet_hydratation_limit) == 0):\n",
    "                \n",
    "                # increment request counter\n",
    "                req_counter = req_counter + 1\n",
    "\n",
    "                # We are done\n",
    "                if(req_counter > nb_requests):\n",
    "                    break\n",
    "                \n",
    "                # Convert to comma separated\n",
    "                ids = list(set(ids))\n",
    "                ids_csv = \",\".join(ids)\n",
    "                \n",
    "                # Clear array\n",
    "                ids = []\n",
    "\n",
    "                try: # If you don't reach the limit of requests\n",
    "                    \n",
    "                    # Send Request for all the ids\n",
    "                    all_status = twitter.lookup_status(id=ids_csv)\n",
    "                    \n",
    "                    # Go through individual status\n",
    "                    for status in all_status:\n",
    "                        \n",
    "                        # init new line\n",
    "                        newline = []\n",
    "                        \n",
    "                        # go through features\n",
    "                        featureExtracted = True\n",
    "                        for feature in features:\n",
    "                            \n",
    "                            if(len(feature) == 1):\n",
    "                                \n",
    "                                # Get datum\n",
    "                                datum = str(status[feature[0]]).strip()\n",
    "                                \n",
    "                                if(feature[0] == 'text'):\n",
    "                                \n",
    "                                    # Clean\n",
    "                                    datum = twitterPreprocessor.clean(datum)\n",
    "                                    \n",
    "                                elif(feature[0] == 'created_at'):\n",
    "                                    \n",
    "                                    # Parse Datetime\n",
    "                                    datum = twitterPreprocessor.to_datetime(datum)\n",
    "                                    \n",
    "                                # Add to line\n",
    "                                newline.append(datum)\n",
    "                                \n",
    "                            elif(len(feature) == 2):\n",
    "                                \n",
    "                                # Get the datum\n",
    "                                datum = str(status[feature[0]][feature[1]]).strip()\n",
    "                                \n",
    "                                if(feature[1] == 'description'):\n",
    "                                    \n",
    "                                    # Clean\n",
    "                                    datum = twitterPreprocessor.clean(datum)\n",
    "                                    \n",
    "                                elif(feature[1] == 'location' and checkLocation == True):\n",
    "                                    \n",
    "                                    if(datum == None):\n",
    "                                        featureExtracted = False\n",
    "                                    elif(len(datum) == 0):\n",
    "                                        featureExtracted = False\n",
    "                                    else:\n",
    "                                        datum = extract_place(datum)\n",
    "                                        if(datum == None):\n",
    "                                            featureExtracted = False\n",
    "                                \n",
    "                                    if(featureExtracted == False):\n",
    "                                        break\n",
    "                                 \n",
    "                                # Add to line\n",
    "                                newline.append(datum)\n",
    "                         \n",
    "                        # Check if all features were extracted\n",
    "                        if(featureExtracted == True):\n",
    "                            \n",
    "                            # Write to output file\n",
    "                            newline = '\"' + '\",\"'.join(newline) + '\"\\n'\n",
    "                            csvfile.write(newline)                        \n",
    "                    \n",
    "\n",
    "                except TwythonError as e:\n",
    "                    if isinstance(e, TwythonRateLimitError):\n",
    "                        retry_after = int(e.retry_after)\n",
    "                        \n",
    "                        print(\"Nbr of requests done : \" + str(req_counter))\n",
    "                        print(\"sleeping for 15 min and 30 seconds\")\n",
    "                        time.sleep(930)\n",
    "                        \n",
    "\n",
    "        print(\"Nbr of requests done : \" + str(req_counter))\n",
    "\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this file as an example : \n",
    "https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/5QCCUU/QPYP8G&version=1.1\n",
    "\n",
    "We only want to keep the content (*text*) and the user ID (*user/screen_name*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = [\"sources/general/election-filter1.txt\",\"sources/general/election-filter2.txt\",\"sources/general/election-filter3.txt\",\"sources/general/election-filter4.txt\",\"sources/general/election-filter5.txt\",\"sources/general/election-filter6.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [['id'],['created_at'],['text'],['user','screen_name'],['user', 'description'],['user','location'],['user','followers_count'],['user','statuses_count'],['retweet_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_REQUEST = 898\n",
    "HYDRATION_LIMIT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Election Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of lines : 50000000\n",
      "Fraction about to be Hydrated : 0.18\n",
      "Nbr of requests done : 899\n"
     ]
    }
   ],
   "source": [
    "collection_id = 4\n",
    "start_index = 28980000\n",
    "start_date = '2016-11-08_2'\n",
    "\n",
    "filename = collections[collection_id]\n",
    "database_name = \"data/\" + start_date + \"_\" + str(start_index) + \"_election-filter\" + str(collection_id+1) + \".csv\"\n",
    "\n",
    "hydrate_database(filename, database_name, features, nb_requests=NBR_REQUEST, tweet_hydratation_limit=HYDRATION_LIMIT, start_index=start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The democratic Party and Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename1 = \"sources/parties_candidates/democratic-candidate-timelines.txt\"\n",
    "filename2 = \"sources/parties_candidates/democratic-party-timelines.txt\"\n",
    "database_name1 = \"data/parties_candidates/democratic/democratic-candidate-timelines.csv\"\n",
    "database_name2 = \"data/parties_candidates/democratic/democratic-party-timelines.csv\"\n",
    "\n",
    "# interchange filename/database\n",
    "hydrate_database(filename1, database_name1, features, nb_requests=899, tweet_hydratation_limit=100, checkLocation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Republican Party and Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"sources/parties_candidates/republican-candidate-timelines.txt\"\n",
    "filename2 = \"sources/parties_candidates/republican-party-timelines.txt\"\n",
    "database_name1 = \"data/parties_candidates/republican/republican-candidate-timelines.csv\"\n",
    "database_name2 = \"data/parties_candidates/republican/republican-party-timelines.csv\"\n",
    "\n",
    "# interchange filename/database\n",
    "hydrate_database(filename1, database_name1, features, nb_requests=899, tweet_hydratation_limit=100, checkLocation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=  glob(\"data/general/2016-11-09/*.csv\")\n",
    "#filenames = ['data/2016-11-07_0_25900000_election-filter5.csv','data/2016-11-07_1_25990000_election-filter5.csv','data/2016-11-07_2_26080000_election-filter5.csv']\n",
    "output_name = 'data/general/2016-11-09/newtweets.csv'\n",
    "\n",
    "concatDocuments(filenames,output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Analysis of a database of your choice  (16 pts)\n",
    "\n",
    "Now that you are able to hydrate a tweets database efficiently and taking into account the limitations of Twitter, you can apply it to the *dataset* that interests you most.\n",
    "\n",
    "### 1. Instructions\n",
    "\n",
    "In this section, you will conduct a **full** Data Science project, from data collection to interpretation of the results. You must choose from the following 3 topics:\n",
    " \n",
    " 1. Sentiment analysis for prediction of the results of the American election.\n",
    "\n",
    "    **Dataset**: \"2016 United States Presidential Election Tweet Ids\", https://doi.org/10.7910/DVN/PDI7IN  \n",
    " \n",
    "    **Note:** This subject is quite similar to TP1 (with here feeling = political party), so you are free to take up what you did. However, we should go a little deeper here, for example on the classification stage. In addition, you have a new problem here which is that your data is not labelled (but the construction of the collections should allow you to label yourself).\n",
    "\n",
    "\n",
    " 2. Hate speech detection.\n",
    "    \n",
    "    **Dataset**: Modify your hyrate function to generate a database with recent tweets, using the `search` function.\n",
    " \n",
    "    **Note:** This subject could also be addressed in the same way as TP1: preprocessing + classification steps. However, in this case, having data with labels \"hateful\"/ \"not hateful\" is much more complex, because many databases labeled, when hydrated, will be almost empty, because the tweets will have been deleted by the time we make our request (because Twitter also ensures that hate tweets are deleted). That's why you have to create a database with the most recent tweets possible, before they are potentially deleted. To designate a tweet as hateful, one method would be to detect hateful vocabulary, for example with `hatebase.org`, which offers large and comprehensive databases. You can create an account on the website to access the API, and then use this library for Python: https://github.com/DanielJDufour/hatebase. If you modify the query to have only tweets containing this vocabulary, and mix it with sentiment analysis, you can get results to analyze. You could also have a \"user\" approach to search for hate tweets: when a tweet is detected as hate, inspect all the user's tweets and/or followers' tweets. To sum up, you have many possibilities, but this subject is probably the most complex one (but I will take it into account). I will therefore be less demanding on 'quantified' results, the important thing here is more the analysis, and having a coherent approach (it is also very important to take the time to think about a clear definition of 'hateful').\n",
    "\n",
    "\n",
    " \n",
    " 3. Clustering methods applied tweets on current events, and analysis of results.\n",
    "\n",
    "    **Dataset**: \"News Outlet Tweet Ids\", https://doi.org/10.7910/DVN/2FIFLH\n",
    "\n",
    "    **Note:** Application of preprocessing methods, then clustering methods to group tweets that mention the same news or news category (your choice!), then visualization, evolution in function of time... You will need to find out what is the best clustering method, and this will depend on your approach (number of classes known? if yes, how many classes?)\n",
    "\n",
    "You are entirely free on the whole process (choice of extracted information, ML methods, library, etc.). As these topics are popular in the scientific community, you can (**if you wish**) draw inspiration from articles in the literature, provided you quote it in your report and make your own implementation. \n",
    "\n",
    "\n",
    "#### The objective here, however, is not to obtain the state of the art, but to apply a clear and rigorous methodology that you have built yourself. \n",
    "\n",
    "Since datasets are massive, it is strongly discouraged to make a database containing all hydrated tweets (for example, the authors of database #1 point out that with the limitations of the API it would take you about 32 days). It is up to you to see what size dataset you need.\n",
    "\n",
    "If you are doing supervised learning: you will need to train a model with a labelled set, and so you have two options available. Either you will have to retrieve labelled data, or you are able to label your data yourself (for example, in the case of subject 1, the database is divided into collections, some of which depend on the political party). You can reuse your TP1 implementation, but you are asked to explore a little more deeply here, especially the classification methods.\n",
    "\n",
    "Also remember to read the README file corresponding to the database you have chosen, in order to help you better understand your future results.\n",
    "\n",
    "### 2. Report\n",
    "\n",
    "For this TP, you will have to provide a report that details and justifies your overall method, and provides the results you have obtained. The following elements must appear (this can be used as a plan, but it is not rigid):\n",
    "\n",
    "- Project title, and name of all team members (with mail and matricule)\n",
    "    \n",
    "- **Introduction** : summary of the problem, methodology and results obtained\n",
    "\n",
    "- **Dataset presentation** : description, justification of size, choice of features, etc. \n",
    "\n",
    "- **Preprocessing** : if any, justification of the preprocessing steps  \n",
    "\n",
    "- **Methodology** : description and justification of all choices (algorithms, hyper-parameters, regularization, metrics, etc.)\n",
    "\n",
    "- **Résults** : analysis of the results obtained (use figures to illustrate), linking design choices to the performance obtained.\n",
    "\n",
    "- **Discussion** : discuss the advantages and disadvantages of your approach; what are the weaknesses, the flaws? What can be improved? You can also suggest future exploration ideas.\n",
    "\n",
    "- **References** : if you have been inspired by a study already done.\n",
    "    \n",
    "You can use the arXiv template for the report : https://fr.overleaf.com/latex/templates/style-and-template-for-preprints-arxiv-bio-arxiv/fxsnsrzpnvwc. **However, the entire report should not exceed 5 pages, including figures and references.** The 5 pages are not mandatory, if you think that less is enough and your report is indeed complete, you will not be penalized.\n",
    "\n",
    "\n",
    "### 3. Expected files\n",
    "\n",
    "At the end of the zip file, you will submit a *zip* file with all the following elements:\n",
    "\n",
    "- The pdf file of the report\n",
    "- This notebook that you will have completed. You can also implement your method further here, or use another file if you wish (the code must be commented and clear).\n",
    "- Do not send the data files because they are too large. With the report and the code, everything will be detailed and it will be possible to do it again easily.\n",
    "\n",
    "### 4. Evalutation\n",
    "\n",
    "75% of the grade (which is 12 points) of this part will be based on the methodology, and 25% (4 points) on the results.\n",
    "\n",
    "The rating on the methodology included: \n",
    "\n",
    "- The relevance of all the steps of the approach\n",
    "\n",
    "- The correct description of the selected algorithms\n",
    "\n",
    "- The judicious justification of the established choices\n",
    "\n",
    "- A relevant analysis of the results\n",
    "\n",
    "- Clarity and organization of the report (figures, tables) and code.\n",
    "\n",
    "\n",
    "As for the results, it is impossible to set a fixed scale because they will depend on the chosen subject. This is a problem you will face: each problem being specific, it can be complicated to qualitatively evaluate a model, especially since you probably do not know the state of the art. That is why it will be important to do several tests, and to compare different methods. Thus, the results must be consistent with the complexity of your implementation: a simple and naive model will provide you with initial results, which you will then have to improve with more precise and complex models.\n",
    "\n",
    "Therefore, all points for the results will be given if: \n",
    " - You obtain first results with a naive method that testify to the relevance of your choices \n",
    " - These results are then significantly improved with a more complex method\n",
    " - All this is well justified and put into the context of the problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
