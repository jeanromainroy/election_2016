{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR : ['label', 'text']\n",
      "Length of training set :  117888\n",
      "Length of validation set :  25879\n",
      "Length of test set :  25371\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('label')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            if label == \"democrat\":\n",
    "                y.append(0)\n",
    "            elif label == \"republican\":\n",
    "                y.append(1)\n",
    "            else:\n",
    "                print(\"ERROR : \" + str(row))\n",
    "                continue\n",
    "                \n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"databases/stemmed.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict Dimension: 163646\n"
     ]
    }
   ],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Init array\n",
    "    bigrams = []\n",
    "    \n",
    "    # Go through tokens\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigrams.append(\" \".join([tokens[i],tokens[i+1]]))\n",
    "    \n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "# Returns unique words\n",
    "def buildDict(tweets, addBigram=False):\n",
    "    \n",
    "    # Init empty set\n",
    "    wordDict = set()\n",
    "    \n",
    "    # Go through each tweet of the validation set\n",
    "    for tweet in tweets:\n",
    "\n",
    "        # Tokenize\n",
    "        words = word_tokenize(tweet)\n",
    "        \n",
    "        # Add Bigram\n",
    "        if(addBigram):\n",
    "            words = words + bigram(words)\n",
    "\n",
    "        # Go through each word\n",
    "        for word in words:\n",
    "\n",
    "            # Append to dictionary if not already there\n",
    "            if(word not in wordDict):\n",
    "                wordDict.add(word)\n",
    "                \n",
    "    # Get the stats\n",
    "    print(\"Dict Dimension: \" + str(len(wordDict)))\n",
    "    \n",
    "    return list(wordDict)\n",
    "\n",
    "\n",
    "# Create a dictionary of all the words\n",
    "wordDict = buildDict(valid_X,addBigram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    # backbone = ResNet50\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Input Layer\n",
    "        self.input = nn.Input()\n",
    "        \n",
    "        # We get the number of features at the output layer of the backbone\n",
    "        num_ftrs = backbone.fc.in_features\n",
    "        \n",
    "        # Fully connected layer at the output of our backbone to reduce the dimensionality\n",
    "        self.backbone.fc = nn.Linear(num_ftrs, HIDDEN_DIM)\n",
    "        \n",
    "        # Randomly zeroes some of the elements of the input tensor with probability p, helps with overfitting\n",
    "        self.dropout1 = nn.Dropout(DROPOUT) \n",
    "        \n",
    "        # Fully connected layer to reduce the dimensionality from HIDDEN_DIM to MAX_LEN\n",
    "        self.linear2 = nn.Linear(HIDDEN_DIM, MAX_LEN)\n",
    "        \n",
    "        # Init a LSTM (input_size, hidden_size)\n",
    "        # input_size = The number of expected features in the input tensor (number of chars)\n",
    "        # hidden_size = The number of features out of our LTSM\n",
    "        # batch_first = makes the output tensors to be provided as (batch, seq, feature). \n",
    "        self.lstm = nn.LSTM(OUTPUT_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        \n",
    "        # The dimensionality of our LSTM gets reduced to fit the number of char\n",
    "        self.out = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "        \n",
    "        \n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((70, ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the LSTM Layer\n",
    "        lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "\n",
    "        return model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # x = tensor containing the images in a batch [32,3,IMG_HEIGHT,IMG_WIDTH]\n",
    "    # target = tensor containing the one-hot encoded labels for these images [32,1,OUTPUT_DIM,MAX_LEN]\n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        # --- We pass the payload through the first layers of our NN ---\n",
    "        \n",
    "        x = self.backbone(x)             # the backbone NN ouputs HIDDEN_DIM features\n",
    "        latent = F.relu(x)               # rectified linear unit (floors at 0)\n",
    "        latent = self.dropout1(latent)   # we randomly set to zero, some elements of the input tensor\n",
    "        length = self.linear2(latent)    # we reduce the number of features to MAX_LEN\n",
    "        \n",
    "        \n",
    "        # --- We pass the payload through our LSTM --- \n",
    "        inputs = torch.zeros(BATCH_SIZE, 1, OUTPUT_DIM).to(self.device)\n",
    "        hidden = (latent.unsqueeze(0), torch.zeros(1, BATCH_SIZE, HIDDEN_DIM).to(self.device)) # (T[1,32,512] , T[1,32,512])\n",
    "        number = []\n",
    "        \n",
    "        for i in range(MAX_LEN):\n",
    "            \n",
    "            output, hidden = self.lstm(inputs, hidden)    # output = T[BATCH_SIZE,1,HIDDEN_DIM]\n",
    "    \n",
    "            # We remove the channel dimension, we have a batch of HIDDEN_DIM features\n",
    "            flatten_output = output[:,-1,:]               # flatten_output = T[BATCH_SIZE,HIDDEN_DIM]\n",
    "            \n",
    "            # We get the proba. distri. of all the potential characters for that batch of HIDDEN_DIM features\n",
    "            digit_prob_distri = self.out(flatten_output)  # T[32,13]\n",
    "            number.append(digit_prob_distri.unsqueeze(0)) # append to number\n",
    "            \n",
    "            # target = T[32,10,13], so we move to a different segment T[32,x,13]\n",
    "            inputs = target[:, i, :].unsqueeze(1)\n",
    "              \n",
    "                \n",
    "        # Reassemble in a T[32,MAX_LEN,OUTPUT_DIM], contains the digit prob distr for every segment of the batch\n",
    "        payload = torch.cat(number, 0).transpose(0, 1)\n",
    "        \n",
    "        return length, payload\n",
    "\n",
    "\n",
    "    # Here we receive the predicted label in the form of indexes of CHARS\n",
    "    def to_num(self, number):\n",
    "        \n",
    "        clean_number = []\n",
    "        \n",
    "        # We go through the indexes and append their corresponding char\n",
    "        for index in number:\n",
    "            clean_number.append(CHARS[index])\n",
    "            \n",
    "        return ''.join(clean_number)\n",
    "    \n",
    "    \n",
    "    # Function to predict the label of an image\n",
    "    def predict(self, x):\n",
    "        \n",
    "        # --- We pass the payload through the first layers of our NN ---\n",
    "        x = self.backbone(x)             # the backbone NN ouputs HIDDEN_DIM features\n",
    "        latent = F.relu(x)               # rectified linear unit (floors at 0)\n",
    "        # ** we dont apply dropout because we are in evaluation mode\n",
    "        \n",
    "        \n",
    "        # --- We pass the payload through our LSTM --- \n",
    "        \n",
    "        inputs = torch.zeros(1, 1, OUTPUT_DIM).to(self.device)\n",
    "        hidden = (latent.unsqueeze(0), torch.zeros(1, 1, HIDDEN_DIM).to(self.device))  \n",
    "        probDistri = torch.zeros([MAX_LEN,OUTPUT_DIM],dtype=torch.float)\n",
    "        \n",
    "        # --- Go through each segment of the image and extract the digit with highest probability ---\n",
    "        number = []\n",
    "        for i in range(MAX_LEN):\n",
    "            output, hidden = self.lstm(inputs, hidden)\n",
    "            \n",
    "            # We get the probability distribution of all the potential characters for that feature segment\n",
    "            digit_prob_distri = self.out(output[:, -1, :])\n",
    "            \n",
    "            # Append to tensor\n",
    "            norm_digit_prob_distri = F.softmax(F.relu(digit_prob_distri),dim=1)\n",
    "            rounded_norm_digit_prob_distri = torch.round(norm_digit_prob_distri*1000.0)/1000.0\n",
    "            probDistri[i] = rounded_norm_digit_prob_distri\n",
    "            \n",
    "            # We get the index of the max probability\n",
    "            index = torch.max(digit_prob_distri, -1)[1][0]\n",
    "            \n",
    "            # If the index points to a blank space we exit the loop (means we are done)\n",
    "            if index == CHARS.index(' '):\n",
    "                break\n",
    "            \n",
    "            # We update the position of our moving feature window (LSTM)\n",
    "            inputs = torch.zeros((1, 1, OUTPUT_DIM)).to(self.device)\n",
    "            inputs[0,0,index] = 1\n",
    "            \n",
    "            # We add the index of the predicted character\n",
    "            number.append(index.item())\n",
    "            \n",
    "        return probDistri, self.to_num(number)\n",
    "    \n",
    "    \n",
    "    # Function to predict the label of an image, but returns null if the level of confidence is not met\n",
    "    def safePredict(self,x,confidenceThresh):\n",
    "    \n",
    "        # Get the prediction\n",
    "        nbrDistri, number = self.predict(x)\n",
    "\n",
    "        # Go through the probability of each character of CHARS\n",
    "        index = 0\n",
    "        for charDistri in nbrDistri:\n",
    "\n",
    "            # Stop if we are after the number\n",
    "            if(index == len(number)):\n",
    "                break\n",
    "                \n",
    "            # Go through the probabilities of all the chars\n",
    "            maxIsValid = False\n",
    "            for prob in charDistri:\n",
    "\n",
    "                # if one item of the prob distribution is higher than the confidence threshold we are good\n",
    "                if(prob.item() >= confidenceThresh):\n",
    "                    maxIsValid = True\n",
    "                    break\n",
    "\n",
    "            # If the digit with maximal probability is under the confidence threshold we exit with false\n",
    "            if(not maxIsValid):\n",
    "                return False\n",
    "            \n",
    "\n",
    "            # Increment char index\n",
    "            index = index + 1\n",
    "            \n",
    "        # If all the NN was confident for all the chars, we return the full number\n",
    "        return number\n",
    "\n",
    "    \n",
    "    # This function prints the probability distribution of the prediction in a nice format\n",
    "    def printPredict(self,x):\n",
    "        \n",
    "        # Get the prediction\n",
    "        nbrDistri, number = self.predict(x)\n",
    "        \n",
    "        # take second element for sort\n",
    "        def takeSecond(elem):\n",
    "            return elem[1]\n",
    "\n",
    "        # Print the number it think it is\n",
    "        print(\"Predicted Nbr : \\t\",number,\"\\n\")\n",
    "\n",
    "        # Go through the probability of each character of CHARS\n",
    "        index = 0\n",
    "        for charDistri in nbrDistri:\n",
    "\n",
    "            # Print the predicted char\n",
    "            if(index < len(number)):\n",
    "                print(\"-\",number[index],\"-\")\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "\n",
    "            # Go through all the probability of the other potential candidates for that char\n",
    "            probs = []\n",
    "            index2 = 0\n",
    "            for prob in charDistri:\n",
    "\n",
    "                # Only consider the predicted char with p > 0.001\n",
    "                if(prob > 0.001):\n",
    "\n",
    "                    # Round to the probability to it's 3rd decimal\n",
    "                    qty = round(prob.item()*1000.0)/1000.0\n",
    "\n",
    "                    # Add to the char candidate and its prob to the array\n",
    "                    probs.append([CHARS[index2],qty])\n",
    "\n",
    "                # increment the index\n",
    "                index2 = index2 + 1\n",
    "\n",
    "            # Sort in descending order the probabilities\n",
    "            orderedProbs = sorted(probs, reverse=True, key=takeSecond)\n",
    "\n",
    "            # Print out the distribution\n",
    "            for prob in orderedProbs:\n",
    "                print(prob[0],\": \",prob[1])\n",
    "\n",
    "            # Carriage return\n",
    "            print(\"\\n\")\n",
    "\n",
    "            # Increment char index\n",
    "            index = index + 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def create_rnn_lstm():\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\" + str(accuracy))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
