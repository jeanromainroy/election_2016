{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('label')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            if label == \"democrat\":\n",
    "                y.append(0)\n",
    "            elif label == \"republican\":\n",
    "                y.append(1)\n",
    "            else:\n",
    "                print(\"ERROR : \" + str(row))\n",
    "                continue\n",
    "                \n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"databases/labeler/stemmed.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Init array\n",
    "    bigrams = []\n",
    "    \n",
    "    # Go through tokens\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigrams.append(\" \".join([tokens[i],tokens[i+1]]))\n",
    "    \n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "# Returns unique words\n",
    "def buildDict(tweets, addBigram=False):\n",
    "    \n",
    "    # Init empty set\n",
    "    wordDict = set()\n",
    "    \n",
    "    # Go through each tweet of the validation set\n",
    "    for tweet in tweets:\n",
    "\n",
    "        # Tokenize\n",
    "        words = word_tokenize(tweet)\n",
    "        \n",
    "        # Add Bigram\n",
    "        if(addBigram):\n",
    "            words = words + bigram(words)\n",
    "\n",
    "        # Go through each word\n",
    "        for word in words:\n",
    "\n",
    "            # Append to dictionary if not already there\n",
    "            if(word not in wordDict):\n",
    "                wordDict.add(word)\n",
    "                \n",
    "    # Get the stats\n",
    "    print(\"Dict Dimension: \" + str(len(wordDict)))\n",
    "    \n",
    "    return list(wordDict)\n",
    "\n",
    "\n",
    "def loadDict():\n",
    "    \n",
    "    # Init dict\n",
    "    wordDict = []\n",
    "    \n",
    "    path = \"databases/dictionary.txt\"\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as input_file:    \n",
    "        for row in input_file:\n",
    "            wordDict.append(row.strip())\n",
    "            \n",
    "    # Get the stats\n",
    "    print(\"Dict Dimension: \" + str(len(wordDict)))\n",
    "            \n",
    "    return wordDict\n",
    "    \n",
    "\n",
    "# Create a dictionary of all the words\n",
    "wordDict = loadDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.words = words\n",
    "        \n",
    "        \n",
    "    def computeLine(self, tweet):\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(tweet)\n",
    "        \n",
    "        # Init the BoW Matrix\n",
    "        matrixBoW = np.zeros((1, len(self.words)),dtype=np.int16)\n",
    "        \n",
    "        # Go through each tokenized tweet\n",
    "        for token in tokens:\n",
    "                \n",
    "            try:\n",
    "                # Get the dictionary index of this token\n",
    "                dictIndex = self.words.index(token)\n",
    "\n",
    "                # Increment the BoW row at this index\n",
    "                matrixBoW[0][dictIndex] += 1\n",
    "\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Return the BoW Matrix\n",
    "        return matrixBoW\n",
    "    \n",
    "        \n",
    "    def computeMatrix(self, tweets):\n",
    "        \"\"\"\n",
    "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
    "        \n",
    "        Entrée: tokens, une liste de vecteurs contenant les tweets (une liste de liste)\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"ERROR: You have not provided the dictionary\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # Init the BoW Matrix\n",
    "        matrixBoW = np.zeros((len(tweets), len(self.words)),dtype=np.int16)\n",
    "        \n",
    "        for i in tqdm(range(0,len(tweets))):\n",
    "            \n",
    "            tweet = tweets[i]\n",
    "            \n",
    "            matrixBoW[i] = self.computeLine(tweet)\n",
    "            \n",
    "        \n",
    "        # Convert to CSR\n",
    "        matrixBoW = csr_matrix(matrixBoW, shape=(len(tweets), len(self.words)), dtype=np.int16)\n",
    "        \n",
    "        # Return the BoW Matrix\n",
    "        return matrixBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBoW = CountBoW(wordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduceDict(wordDict,countBoW,tweets):\n",
    "\n",
    "    # Init the BoW Matrix\n",
    "    matrixBoW = np.zeros((1, len(wordDict)),dtype=np.int16)\n",
    "\n",
    "    for tweet in tqdm(tweets):\n",
    "\n",
    "        # Compute the BoW\n",
    "        bowObject = countBoW.computeLine(tweet)\n",
    "\n",
    "        # Add to matrixBoW\n",
    "        matrixBoW = np.add(matrixBoW,bowObject)\n",
    "\n",
    "\n",
    "    # Only keep words that occured more than once\n",
    "    newWordDict = []\n",
    "    for ind in np.argwhere(matrixBoW > 1):\n",
    "        newWordDict.append(wordDict[ind[1]])\n",
    "\n",
    "    reduction = len(newWordDict)/(1.0*len(wordDict))\n",
    "    print(reduction)\n",
    "\n",
    "\n",
    "    # Write to file\n",
    "    path = \"databases/dictionary.txt\"\n",
    "    with open(path, 'w+', newline='', encoding=\"utf-8\") as output_file:\n",
    "        for word in newWordDict:\n",
    "            output_file.write(str(word) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Load Model\n",
    "def loadModel():\n",
    "    \n",
    "    try:\n",
    "        classifier = load('logistic.joblib') \n",
    "        return classifier\n",
    "    \n",
    "    except:\n",
    "        print(\"Model not saved\")\n",
    "        \n",
    "        \n",
    "def saveModel(clf):\n",
    "    \n",
    "    dump(clf, 'logistic.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1,solver='lbfgs', multi_class='auto')\n",
    "\n",
    "    training_rep = bowObj.computeMatrix(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.computeMatrix(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBoW = CountBoW(wordDict)\n",
    "classifier, trainAcc, validationAcc = train_evaluate(train_X,train_Y,valid_X,valid_Y,countBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy: \" + str(trainAcc))\n",
    "print(\"Validation Accuracy: \" + str(validationAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet,label):\n",
    "\n",
    "    # Make a prediction\n",
    "    pred = classifier.predict(countBoW.computeLine(tweet))\n",
    "\n",
    "    # Print tweet\n",
    "    print(tweet)\n",
    "\n",
    "    # Print prediction\n",
    "    if(pred[0] == 0):\n",
    "        print(\"Pred: Democrat\")\n",
    "    else:\n",
    "        print(\"Pred: Republican\")\n",
    "\n",
    "    # Print Actual\n",
    "    if(label == 0):\n",
    "        print(\"Actual: Democrat\")\n",
    "    else:\n",
    "        print(\"Actual: Republican\")\n",
    "        \n",
    "    return pred[0] == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance\n",
    "preds = classifier.predict(countBoW.computeMatrix(train_X[1:4]))\n",
    "np.count_nonzero(np.equal(preds,train_Y))/len(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
