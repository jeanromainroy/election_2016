{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, quotechar='\"', delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('label')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            if label == \"democrat\":\n",
    "                y.append(0)\n",
    "            elif label == \"republican\":\n",
    "                y.append(1)\n",
    "            else:\n",
    "                print(\"ERROR : \" + str(row))\n",
    "                continue\n",
    "                \n",
    "            x.append(row[ind_text])\n",
    "            \n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"databases/stemmed.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # Init array\n",
    "    bigrams = []\n",
    "    \n",
    "    # Go through tokens\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigrams.append(\" \".join([tokens[i],tokens[i+1]]))\n",
    "    \n",
    "    # This function returns the list of bigrams\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "# Returns unique words\n",
    "def buildDict(tweets, addBigram=False):\n",
    "    \n",
    "    # Init empty set\n",
    "    wordDict = set()\n",
    "    \n",
    "    # Go through each tweet of the validation set\n",
    "    for tweet in tweets:\n",
    "\n",
    "        # Tokenize\n",
    "        words = word_tokenize(tweet)\n",
    "        \n",
    "        # Add Bigram\n",
    "        if(addBigram):\n",
    "            words = words + bigram(words)\n",
    "\n",
    "        # Go through each word\n",
    "        for word in words:\n",
    "\n",
    "            # Append to dictionary if not already there\n",
    "            if(word not in wordDict):\n",
    "                wordDict.add(word)\n",
    "                \n",
    "    # Get the stats\n",
    "    print(\"Dict Dimension: \" + str(len(wordDict)))\n",
    "    \n",
    "    return list(wordDict)\n",
    "\n",
    "\n",
    "# Create a dictionary of all the words\n",
    "wordDict = buildDict(train_X,addBigram=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.words = words\n",
    "        \n",
    "    def computeLineBoW(self, tokens):\n",
    "        \n",
    "        # Init the BoW Matrix\n",
    "        matrixBoW = np.zeros((1, len(self.words)),dtype=np.int16)\n",
    "        \n",
    "        # Go through each tokenized tweet\n",
    "        for token in tokens:\n",
    "                \n",
    "            try:\n",
    "                # Get the dictionary index of this token\n",
    "                dictIndex = self.words.index(token)\n",
    "\n",
    "                # Increment the BoW row at this index\n",
    "                matrixBoW[0][dictIndex] += 1\n",
    "\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Return the BoW Matrix\n",
    "        return matrixBoW\n",
    "    \n",
    "        \n",
    "    def computeBoW(self, tokens):\n",
    "        \"\"\"\n",
    "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
    "        \n",
    "        Entrée: tokens, une liste de vecteurs contenant les tweets (une liste de liste)\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"ERROR: You have not provided the dictionary\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # Init the BoW Matrix\n",
    "        matrixBoW = np.zeros((len(tokens), len(self.words)),dtype=np.int16)\n",
    "        \n",
    "        # Go through each tokenized tweet\n",
    "        for i in range(0,len(tokens)):\n",
    "            \n",
    "            # Get tokenized tweet\n",
    "            tokenizedTweet = tokens[i]\n",
    "            \n",
    "            # Go through each tokens\n",
    "            for j in range(0,len(tokenizedTweet)):\n",
    "                \n",
    "                # Get the token\n",
    "                token = tokenizedTweet[j]\n",
    "                \n",
    "                try:\n",
    "                    # Get the dictionary index of this token\n",
    "                    dictIndex = self.words.index(token)\n",
    "                    \n",
    "                    # Increment the BoW row at this index\n",
    "                    matrixBoW[i][dictIndex] += 1\n",
    "                    \n",
    "                except ValueError:\n",
    "                    pass\n",
    "        \n",
    "        # Return the BoW Matrix\n",
    "        return matrixBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countBoW = CountBoW(wordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init the BoW Matrix\n",
    "matrixBoW = np.zeros((1, len(wordDict)),dtype=np.int16)\n",
    "\n",
    "for tweet in tqdm(train_X):\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_tweet = word_tokenize(tweet)\n",
    "    \n",
    "    # Compute the BoW\n",
    "    bowObject = countBoW.computeLineBoW(tokenized_tweet)\n",
    "    \n",
    "    # Add to matrixBoW\n",
    "    matrixBoW = np.add(matrixBoW,bowObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep words that occured more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newWordDict = []\n",
    "for ind in np.argwhere(matrixBoW > 1):\n",
    "    newWordDict.append(wordDict[ind[1]])\n",
    "    \n",
    "reduction = len(newWordDict)/(1.0*len(wordDict))\n",
    "print(reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "path = \"databases/dictionary.txt\"\n",
    "with open(path, 'w+', newline='', encoding=\"utf-8\") as output_file:\n",
    "    for word in newWordDict:\n",
    "        output_file.write(str(word) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "class Dataset(torch.utils.data.dataset.Dataset):\n",
    "    \n",
    "    def __init__(self, tweets, labels):\n",
    "        \n",
    "        # We set the images attribute as the filename column of the csv\n",
    "        self.tweets = tweets\n",
    "        \n",
    "        # We set the labels attribute as the value column of the csv\n",
    "        self.labels = labels\n",
    "        \n",
    "        # The length is simply the number of rows\n",
    "        self.length = len(self.tweets)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Get Tweet\n",
    "        tweet = self.tweets[index]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized_tweet = word_tokenize(tweet)\n",
    "        \n",
    "        # Compute the BoW\n",
    "        bowObject = countBoW.computeBoW(tokenized_tweet)\n",
    "        \n",
    "        # Get the label for this image\n",
    "        label = str(self.labels[index])\n",
    "            \n",
    "        return {\"text\":bowObject, \"label\":label}\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
