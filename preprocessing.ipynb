{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import sys\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Pytorch Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# !pip install --user tweet-preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpher Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    \n",
    "    nbrOfLines = i + 1\n",
    "    print(\"Nbr of lines : \" + str(nbrOfLines))\n",
    "    \n",
    "    return nbrOfLines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "def clean_tweets(tweet):\n",
    " \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', tweet) # remove URLs\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)#remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)#filter using NLTK library append it to a string\n",
    "    \n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_tweet = [] #looping through conditions\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        \n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation and not hasNumbers(w):\n",
    "            filtered_tweet.append(w)\n",
    "       \n",
    "    # Join tokens\n",
    "    tweet = ' '.join(filtered_tweet)\n",
    "    \n",
    "    tweet = tweet.strip()\n",
    "            \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['democrat','republican','neutral']\n",
    "features = ['text','description']\n",
    "\n",
    "# We have the pro-democrat tweets and pro-republican tweets\n",
    "# Each of them are composed of tweets from the candidates and party twitter account\n",
    "democratic_path = \"databases/democratic/democratic.csv\"\n",
    "republican_path = \"databases/republican/republican.csv\"\n",
    "\n",
    "# We want to merge the two files with only the needed features & labels\n",
    "output_path = \"databases/sources.csv\"\n",
    "\n",
    "# Creation of the file that will contain the hydrated tweets:\n",
    "with open(output_path, 'w+', newline='', encoding=\"utf-8\") as output_file:\n",
    "    \n",
    "    # --- Democratic ---\n",
    "    with open(democratic_path, 'r', newline='', encoding=\"latin-1\") as input_file:\n",
    "\n",
    "        # init reader\n",
    "        reader = csv.reader(input_file, quotechar='\"', delimiter=',')\n",
    "\n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_text = header.index('text')\n",
    "        ind_description = header.index('description')\n",
    "        \n",
    "        # Write headers for first row\n",
    "        output_file.write('\"label\",\"text\",\"description\"\\n')\n",
    "\n",
    "        # Go through input file\n",
    "        for row in reader:\n",
    "            \n",
    "            # Preprocess the content\n",
    "            text = clean_tweets(row[ind_text])\n",
    "            description = clean_tweets(row[ind_description])\n",
    "            \n",
    "            rowData = [\"democrat\",text,description]\n",
    "            rowData = '\"' + '\",\"'.join(rowData) + '\"\\n'\n",
    "            \n",
    "            # Write row\n",
    "            output_file.write(rowData)\n",
    "            \n",
    "    \n",
    "    # --- Republicains ---    \n",
    "    with open(republican_path, 'r', newline='', encoding=\"latin-1\") as input_file:\n",
    "\n",
    "        # init reader\n",
    "        reader = csv.reader(input_file, quotechar='\"', delimiter=',')\n",
    "\n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_text = header.index('text')\n",
    "        ind_description = header.index('description')\n",
    "        \n",
    "        # Write headers for first row\n",
    "        output_file.write(\"label,text,description\\n\")\n",
    "\n",
    "        # Go through input file\n",
    "        for row in reader:\n",
    "            \n",
    "            # Preprocess the content\n",
    "            text = clean_tweets(row[ind_text])\n",
    "            description = clean_tweets(row[ind_description])\n",
    "            \n",
    "            rowData = [\"republican\",text,description]\n",
    "            rowData = '\"' + '\",\"'.join(rowData) + '\"\\n'\n",
    "            \n",
    "            # Write row\n",
    "            output_file.write(rowData)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        token: a string that contain a token\n",
    "        \"\"\"\n",
    "        \n",
    "        # Have to return the stemmed token\n",
    "        return [self.stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"databases/sources.csv\"\n",
    "output_path = \"databases/stemmed.csv\"\n",
    "\n",
    "# Creation of the file that will contain the hydrated tweets:\n",
    "with open(output_path, 'w+', newline='', encoding=\"utf-8\") as output_file:\n",
    "    with open(input_path, 'r', newline='', encoding=\"utf-8\") as input_file:\n",
    "        \n",
    "        # init reader\n",
    "        reader = csv.reader(input_file, quotechar='\"', delimiter=',')\n",
    "\n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('label')\n",
    "        ind_text = header.index('text')\n",
    "        ind_description = header.index('description')\n",
    "        \n",
    "        # Write headers\n",
    "        output_file.write('\"label\",\"text\"\\n')\n",
    "        \n",
    "        # Init stemmer\n",
    "        stemmer = Stemmer()\n",
    "        \n",
    "        # Go through input file\n",
    "        for row in reader:\n",
    "            \n",
    "            # Get the content\n",
    "            label = row[ind_label]\n",
    "            text = row[ind_text]\n",
    "            \n",
    "            # Make sure there is no punctuation\n",
    "            text = ''.join([c for c in text if c not in string.punctuation])\n",
    "            \n",
    "            # Tokenize\n",
    "            text_tokens = word_tokenize(text)            \n",
    "            \n",
    "            # Stem\n",
    "            stems = stemmer.stem(text_tokens)\n",
    "            \n",
    "            # Join stems\n",
    "            stems = \" \".join(stems)\n",
    "            \n",
    "            # Write row            \n",
    "            rowData = [label,stems]\n",
    "            rowData = '\"' + '\",\"'.join(rowData) + '\"\\n'\n",
    "            \n",
    "            # Write row\n",
    "            output_file.write(rowData)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
